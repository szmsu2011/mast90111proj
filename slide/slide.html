<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>MAST90111 Advanced Statistical Modelling</title>
    <meta charset="utf-8" />
    <meta name="author" content="Stephen Su, Thomas Black" />
    <script src="slide_files/header-attrs/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# MAST90111 Advanced Statistical Modelling
]
.subtitle[
## Predicting Salary of Major League Baseball Players
]
.author[
### Stephen Su, Thomas Black
]
.date[
### October 2024
]

---








## Introduction

- Aim: predict **salary** (log scaled) of major league baseball players by cumulated **hits** and **years** in profession.

- Assumption: all variables above are continuous on `\(\mathbb{R}\)`. Also, all conditions mentioned in lecture hold.

- A data set of 263 major league players from the 1986 and 1987 seasons (James et al., 2013).

- Randomly sampled: 200 observations in training set, 63 in test set.


``` r
set.seed(90111)
train &lt;- slice_sample(data, n = 200)
test &lt;- setdiff(data, train)
```

- Nonparametric/semiparametric models are fitted, using the data in the training set, as well as their conventional parametric counterparts.

- The predictive performance of the methods are evaluated and compared, using the data in the test set.

---

## Univariate prediction

- Suppose only the log **salary** variable `\(Y\)` is observable.

- Sample mean: best point predictor in terms of `\(\ell_2\)`-loss.

- Interval predictor: suppose `\(\hat{f}\)` is an estimate of the p.d.f. of `\(Y\)`, then a central `\((1 - \alpha)\)` confidence interval of prediction is

`$$\left\{[a, b] : \int_{-\infty}^a \hat{f}(x)dx = \frac{\alpha}{2}, \int_{-\infty}^b \hat{f}(x)dx = 1 - \frac{\alpha}{2}\right\}$$`

- If we assume Normality (a parametric approach): `\(Y \sim \text{N}(\mu, \sigma^2)\)` then

`$$\hat{f}(x) = \phi\left(\frac{x - \hat\mu}{\hat\sigma}\right)$$`

- Kernel density estimation (a nonparametric approach)

`$$\hat{f}(x) = \frac{1}{nh} \sum_{i = 1}^n K\left(\frac{x - Y_i}{h}\right) \text{ for some } h &gt; 0$$`

---

## Univariate prediction: evaluation

- Let `\(p\)` be the proportion of **test** data points included inside the **trained** 50% central confidence interval, we have the exact Binomial test (Clopper &amp; Pearson, 1934) given by

`$$H_0: p = 0.5; H_1: p \neq 0.5$$`

- Normal: `\(35\)`% (95% CI: `\([0.23, 0.48]\)`, p-value: `\(0.0226\)`).

- KDE (ROT and LSCV): `\(44\)`% (95% CI: `\([0.32, 0.58]\)`, p-value: `\(0.45\)`).


&lt;img src="slide_files/figure-html/univar-fit-plot-1.png" width="2800" style="display: block; margin: auto;" /&gt;

---

## Univariate prediction: evaluation

- The result obtained above subjects to the variability of the sampling process of training/test data. We thus repeat the sampling, fitting and evaluation process `\(m = 100\)` times.

&lt;img src="slide_files/figure-html/univar-repeat-eval-1.png" width="2800" style="display: block; margin: auto;" /&gt;

---

## Bivariate regression

- Suppose we also observe the cumulated number of **hits** `\(X_1\)`.

- We wish to estimate `\(m(x) = \mathbb{E}(Y | X_1 = x)\)`.

- For observation `\((X_{n + 1}, Y_{n + 1})\)`, use `\(\hat{m}(X_{n + 1})\)` as the predictor for the "unknown" (test) `\(Y_{n + 1}\)`.

- Assume Normality - linear regression (a parametric approach); by OLS:

`$$m(x) = \beta_0 + \beta_1x$$`

- Local linear regression (a nonparametric approach): `\(p = 1\)` as we are estimating the zero derivative; by LSCV:

`$$m(x) \approx m(x_0) + m'(x_0)(x - x_0)$$`

- Penalised cubic spline regression (a nonparametric approach): with `\(L = \min\{200 / 4, 35\} = 35\)`; by GCV:

`$$m(x) = s(x)$$`

---

## Bivariate regression: evaluation


&lt;img src="slide_files/figure-html/bivar-reg-fit-plot-1.png" width="2800" style="display: block; margin: auto;" /&gt;

|          |          |             |                 |
|:---------|:---------|:------------|:----------------|
|model     |Linear    |Local linear |Penalised spline |
|Test MSEP |0.5995660 |0.5995660    |0.5407159        |

---

## Bivariate regression: evaluation

- The result obtained above subjects to the variability of the sampling process of training/test data. An example is the local linear model and the linear regression coincides by chance. We thus repeat the sampling, fitting and evaluation process `\(m = 100\)` times.


&lt;img src="slide_files/figure-html/bivar-repeat-eval-plot-1.png" width="2800" style="display: block; margin: auto;" /&gt;

---

## Multivariate regression

- Suppose we also observe the **years** played in major league `\(X_2\)`.

- Thus we wish to estimate `\(m(\mathbf{x}) = \mathbb{E}(Y | \mathbf{X} = \mathbf{x})\)`.

- For a new `\((\mathbf{X}_{n + 1}, Y_{n + 1})\)`, `\(\hat{m}(\mathbf{X}_{n + 1})\)` is the predictor for `\(Y_{n + 1}\)`.

- Assume Normality - linear regression (a parametric approach); by OLS:

`$$m(\mathbf{x}) = \mathbf{x}^\top\boldsymbol\beta$$`

- Single index model (a semiparametric approach) via penalised spline:

`$$m(\mathbf{x}) = s(\mathbf{x}^\top\boldsymbol\beta)$$`

- Partial linear model (a semiparametric approach) with linear `\(X_1\)`:

`$$m(\mathbf{x}) = \beta_0 + \beta_1 x_1 + s(x_2)$$`

- Additive model (a semiparametric approach):

`$$m(\mathbf{x}) = \beta_0 + s_1(x_1) + s_2(x_2)$$`

---

## Multivariate regression: evaluation

&lt;img src="slide_files/figure-html/multivar-repeat-eval-1.png" width="2800" style="display: block; margin: auto;" /&gt;

---

## Open and reproducible work

The source files and version histories are available on **Github**.

**Install required dependencies (R console)**

```r
install.packages("tidyverse")
install.packages("xaringan")
install.packages("xaringanthemer")
install.packages("np")
install.packages("mgcv")
install.packages("tinytex")
tinytex::install_tinytex()
```

**Usage (command line)**

```zsh
git clone https://github.com/szmsu2011/mast90111proj
cd mast90111proj
R
```
```r
rmarkdown::render("report/report.Rmd")
rmarkdown::render("slide/slide.Rmd")
```

---

## References

- Clopper, C. J., &amp; Pearson, E. S. (1934). The use of confidence or fiducial limits illustrated in the case of the binomial. *Biometrika*, *26*(4), 404-413. [https://doi.org/10.2307/2331986](https://doi.org/10.2307/2331986)

- James, G., Witten, D., Hastie, T., Tibshirani, R., et al. (2013). *An introduction to statistical learning*. Springer.

- Wood, S. N. (2011). Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. *Journal of the Royal Statistical Society (B)*, *73*(1), 3-36. [https://doi.org/10.1111/j.1467-9868.2010.00749.x](https://doi.org/10.1111/j.1467-9868.2010.00749.x)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
